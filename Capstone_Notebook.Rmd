---
title: "Coursera R DS Spec. Capstone Project"
author: "Felix m0wlwurf"
date: "13 12 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r}
library(tm)
library(RWeka)
library(slam)
library(word2vec)
```

## Loading the Data

```{r}
setwd("~/R-Course/CourseraCapstone")

con <- file("./final/en_US/en_US.blogs.txt")
en_US.blogs <- readLines(con)
close(con)

con <- file("./final/en_US/en_US.news.txt")
en_US.news <- readLines(con)
close(con)

con <- file("./final/en_US/en_US.twitter.txt")
en_US.twitter <- readLines(con)
close(con)
```

## Preparing Data for further processing

### Corpus-Format

In order to further process the data (i.e. use tokenization methods, ngrams etc) we need to bring it in an appropriate format. It is common to store text data in Corpora. We can use the tm-Package for this task.Note that "Corpus()" seems to create an object of class "simplecorpus", which is good for memory but does not allow for more advanced tasks like ngrams.

```{r}
corp.en_US.blogs <- Corpus(VectorSource(en_US.blogs)) #VCorpus() for more advanced operations, but memory limitations kick in..
corp.en_US.news <- Corpus(VectorSource(en_US.news))
corp.en_US.twitter <- Corpus(VectorSource(en_US.twitter))
```

### TermDocumentMatrix

From the Corpora, TermDocumentMatrices can be created. These store the frequencies of each occuring word of a document on a "per-document basis". Further analysis can start from there.

```{r}
tdm.en_US.blogs <- TermDocumentMatrix(corp.en_US.blogs)
tdm.en_US.news <- TermDocumentMatrix(corp.en_US.news)
tdm.en_US.twitter <- TermDocumentMatrix(corp.en_US.twitter)
```


## Exploratory Data Analysis

### Single Word Frequencies for each of the Corpora

From the TermDocumentMatrix, the Frequency of each Word in the overall Corpus can be derived. To make it easier to access the most frequent words, the result is frequency sorted:
```{r}
term_freq.en_US.blogs <- sort(slam::row_sums(tdm.en_US.blogs), decreasing = TRUE)
term_freq.en_US.blogs <- term_freq.en_US.blogs / sum(term_freq.en_US.blogs)

term_freq.en_US.news <- sort(slam::row_sums(tdm.en_US.news), decreasing = TRUE)
term_freq.en_US.news <- term_freq.en_US.news / sum(term_freq.en_US.news)

term_freq.en_US.twitter <- sort(slam::row_sums(tdm.en_US.twitter), decreasing = TRUE)
term_freq.en_US.twitter <- term_freq.en_US.twitter / sum(term_freq.en_US.twitter)
```

### How many unique words cover 50% / 90% of the overall word occurences?
```{r}
cum_term_freq.en_US.blogs <- cumsum(term_freq.en_US.blogs)
i50.en_US.blogs <- max(which(cum_term_freq.en_US.blogs < .5)) #how many words to cover 50% of coverage?
i90.en_US.blogs <- max(which(cum_term_freq.en_US.blogs < .9)) #how many words to cover 90% of coverage?
print(paste("en_US.blogs: in order cover 50% of overall word occurences", i50.en_US.blogs, "unique words of", length(cum_term_freq.en_US.blogs), "the total unique words are needed" ))
print(paste("en_US.blogs: in order cover 90% of overall word occurences", i90.en_US.blogs, "unique words of", length(cum_term_freq.en_US.blogs), "the total unique words are needed" ))

cum_term_freq.en_US.news <- cumsum(term_freq.en_US.news)
i50.en_US.news <- max(which(cum_term_freq.en_US.news < .5))
i90.en_US.news <- max(which(cum_term_freq.en_US.news < .9))
print(paste("en_US.news: in order cover 50% of overall word occurences", i50.en_US.news, "unique words of", length(cum_term_freq.en_US.news), "the total unique words are needed" ))
print(paste("en_US.news: in order cover 90% of overall word occurences", i90.en_US.news, "unique words of", length(cum_term_freq.en_US.news), "the total unique words are needed" ))

cum_term_freq.en_US.twitter <- cumsum(term_freq.en_US.twitter)
i50.en_US.twitter <- max(which(cum_term_freq.en_US.twitter < .5))
i90.en_US.twitter <- max(which(cum_term_freq.en_US.twitter < .9))
print(paste("en_US.twitter: in order cover 50% of overall word occurences", i50.en_US.twitter, "unique words of", length(cum_term_freq.en_US.twitter), "the total unique words are needed" ))
print(paste("en_US.twitter: in order cover 90% of overall word occurences", i90.en_US.twitter, "unique words of", length(cum_term_freq.en_US.twitter), "the total unique words are needed" ))

word.coverage <- data.frame(Corpus = c("en_US.blogs", "en_US.news", "en_US.twitter"), Words.to.Cover.50p = c(i50.en_US.blogs, i50.en_US.news, i50.en_US.twitter), Words.to.Cover.90p = c(i90.en_US.blogs, i90.en_US.news, i90.en_US.twitter), Total.Unique.Words = c(length(cum_term_freq.en_US.blogs),
                                                                                                                                                                                                                                                                     length(cum_term_freq.en_US.news),                                                                                                                                                                                                                                                                       length(cum_term_freq.en_US.twitter)))
print(word.coverage)
```


```{r}
plot(cum_term_freq.en_US.blogs[y = seq(1, length(cum_term_freq.en_US.blogs), by = length(cum_term_freq.en_US.blogs) / 1000)], x = seq(1, length(cum_term_freq.en_US.blogs), by = length(cum_term_freq.en_US.blogs) / 1000), 
     type = "l", lwd = 2, col = "red", ylab = "cumulative sum over percentage", xlab = "Index", main = "en_US.blogs")
abline(h = .5, col = "green", lwd = 2)
abline(h = .9, col = "blue", lwd = 2)

plot(cum_term_freq.en_US.news[seq(1, length(cum_term_freq.en_US.news), by = length(cum_term_freq.en_US.news) / 1000)], x = seq(1, length(cum_term_freq.en_US.news), by = length(cum_term_freq.en_US.news) / 1000), 
     type = "l", lwd = 2, col = "red", ylab = "cumulative sum over percentage", xlab = "Index", main = "en_US.news")
abline(h = .5, col = "green", lwd = 2)
abline(h = .9, col = "blue", lwd = 2)

plot(cum_term_freq.en_US.twitter[seq(1, length(cum_term_freq.en_US.twitter), by = length(cum_term_freq.en_US.twitter) / 1000)], x = seq(1, length(cum_term_freq.en_US.twitter), by = length(cum_term_freq.en_US.twitter) / 1000), 
     type = "l", lwd = 2, col = "red", ylab = "cumulative sum over percentage", xlab = "Index", main = "en_US.twitter")
abline(h = .5, col = "green", lwd = 2)
abline(h = .9, col = "blue", lwd = 2)
```

The distribution seems to be different in the en_US.news Corpus than in the other two, i.e. the initial slope is lower. This could be due to the difference of language in news articles in comparison to the language used in a 150 letter tweet. Especially, the total number of unique words in the en_US.news Corpus is only ~20% of the other Corpora. Maybe the News-Language is more formalized and contains less variety. It could also be due to spelling mistakes in the non-professional Corpora.

In all cases, a very reduced set of words will cover 90% of the occurences. This finding should be used to compress the data later on.

### What are the most frequent words in the corpora?

```{r}
most.frequent <- data.frame(Corpus=c("en_US.blogs", "en_US.news", "en_US.twitter"), most.frequent.words = c(paste(names(term_freq.en_US.blogs[1:50]), collapse = "; "), paste(names(term_freq.en_US.news[1:50]), collapse = "; "), paste(names(term_freq.en_US.twitter[1:50]), collapse = "; ")))
print(most.frequent)
barplot(term_freq.en_US.blogs[1:50], main = "en_US.blogs", las = 2, cex.names = .8)
barplot(term_freq.en_US.news[1:50], main = "en_US.news", las = 2, cex.names = .8)
barplot(term_freq.en_US.twitter[1:50], main = "en_US.twitter", las = 2, cex.names = .8)

```
### Summary Analysis over all three Corpora

```{r}
raw.all <- c(en_US.blogs, en_US.news, en_US.twitter)
corp.en_US.all <- Corpus(VectorSource(raw.all))
tdm.en_US.all <- TermDocumentMatrix(corp.en_US.all)
term_freq.en_US.all <- sort(slam::row_sums(tdm.en_US.all), decreasing = TRUE)
term_freq.en_US.all <- term_freq.en_US.all / sum(term_freq.en_US.all)

cum_term_freq.en_US.all <- cumsum(term_freq.en_US.all)
i50.en_US.all <- max(which(cum_term_freq.en_US.all < .5))
i90.en_US.all <- max(which(cum_term_freq.en_US.all < .9))

plot(cum_term_freq.en_US.all[seq(1, length(cum_term_freq.en_US.all), by = length(cum_term_freq.en_US.all) / 1000)], x = seq(1, length(cum_term_freq.en_US.all), by = length(cum_term_freq.en_US.all) / 1000), 
     type = "l", lwd = 2, col = "red", ylab = "cumulative sum over percentage", xlab = "Index", main = "en_US.all")
abline(h = .5, col = "green", lwd = 2)
abline(h = .9, col = "blue", lwd = 2)

barplot(term_freq.en_US.all[1:50], main = "en_US.all", las = 2, cex.names = .8)
print(paste(names(term_freq.en_US.all[1:50]),collapse = "; "))
word.coverage.all <- data.frame(Corpus = "en_US.all", Words.to.Cover.50p = i50.en_US.all, Words.to.Cover.90p = i90.en_US.all, Total.Unique.Words = length(cum_term_freq.en_US.all))
word.coverage <- rbind(word.coverage, word.coverage.all)
print(word.coverage)
sum(word.coverage$Total.Unique.Words[1:3])
```
Noteworthy: The Number of unique words in the "all"-Corpus is 1816355, while the sum of unique words of the three sub-corpora is 2219020. This means there is very little overlap of only 18% in terms of unique words between the corpora. This could be due to occurence of "single-use" words, which only appear once in the Corpora. As shown above, the 50 most frequent words in the Corpora are more or less the same. These single use-words might be later on neglected as there significance might be very low and they will make for a lot the overall memory requirement if included.

### 2gram and 3gram Analysis

In order to use n-gram recognition, we need to store the text data in the VCorpus Format (Simple Corpus as used above is not supported). VCorpus seems to use a lot more memory. Therefore the overall corpus need to be split in sub-corpora. From trials, splitting in 10 sub-Corpora seems feasable.

```{r}
tfm.3gram <- TermDocumentMatrix(VCorpus(VectorSource(NULL)))

for (i in 1:10){
    l <- length(raw.all)
    it <- 1:(l/10) + (i-1)*(l/10)
    corp.sub <- VCorpus(VectorSource(raw.all[it]))
    
    btm <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
    tdm.3gram <- c(tfm.3gram, TermDocumentMatrix(corp.sub, control = list(tokenize = btm)))
    print(paste("ngram-tdm: run",i, "/10 finished"))
}

term_freq.3gram <- sort(slam::row_sums(tfm.3gram), decreasing = TRUE)
term_freq.3gram <- term_freq.3gram / sum(term_freq.3gram)

cum_term_freq.3gram <- cumsum(term_freq.3gram)
plot(cum_term_freq.3gram[seq(1, length(cum_term_freq.3gram), by = length(cum_term_freq.3gram) / 1000)], x = seq(1, length(cum_term_freq.3gram), by = length(cum_term_freq.3gram) / 1000), 
     type = "l", lwd = 2, col = "red", ylab = "cumulative sum over percentage", xlab = "Index", main = "en_US.all")
abline(h = .5, col = "green", lwd = 2)
abline(h = .9, col = "blue", lwd = 2)

barplot(term_freq.3gram[1:50], main = "3gram.sub", las = 2, cex.names = .8)
``` 

``